# Test Coverage Improvement Plan

## Overview

This document outlines a comprehensive plan to improve test coverage in the ClaudeStep project by implementing Python testing best practices. The goal is to achieve robust, maintainable test coverage that enables confident refactoring and prevents regressions.

**‚ö†Ô∏è IMPORTANT**: All tests written for this project MUST follow the **Test Style Guide and Conventions** section below. Consistency in test style is critical for maintainability, readability, and long-term project health. Code reviews will enforce adherence to these standards.

## Recent Updates (December 2025)

### Architecture Modernization (December 27, 2025)
Following the completion of the architecture modernization (see `docs/completed/architecture-update.md`), this test plan has been updated to reflect:

1. **Layered Architecture** - Code reorganized into domain, infrastructure, application, and CLI layers
2. **Test Structure Reorganized** - Tests now mirror the `src/` structure in `tests/unit/`
3. **All 112 Tests Passing** - Fixed failing `test_prepare_summary.py` tests
4. **CI Workflow Added** - Unit tests run automatically on push and PR
5. **E2E Tests Updated** - Demo repository tests working with new architecture

### Branch Naming Simplification
Following the completion of the branch naming simplification refactoring (see `docs/completed/simplify-branch-naming.md`):

1. **New `pr_operations.py` module** - Centralized PR utilities with comprehensive tests (21 test cases) already implemented
2. **Simplified branch naming** - All tests use the standard format `claude-step-{project}-{index}`
3. **Removed `branchPrefix` configuration** - Tests verify this field is rejected with a helpful error message
4. **Simplified `project_detection.py`** - Uses centralized `parse_branch_name()` utility
5. **Centralized PR fetching** - Multiple modules now use shared `get_project_prs()` utility

These changes reduce code duplication and simplify the testing surface area.

## Current State

### Test Infrastructure ‚úÖ
- `pytest.ini` - Pytest configuration
- `.github/workflows/test.yml` - CI workflow for unit tests
- `pyproject.toml` - Package configuration with test dependencies
- Tests run on every push and PR to main branch
- **493 tests passing** with 0 failures (up from 486)
- **Coverage: 85.03%** (exceeding 70% minimum threshold)

### Existing Tests (Organized by Layer)
**Domain Layer:**
- `tests/unit/domain/test_exceptions.py` - Custom exception classes (17 tests)
- `tests/unit/domain/test_models.py` - Domain models and formatters (37 tests)
- `tests/unit/domain/test_config.py` - Configuration loading and validation (26 tests)

**Infrastructure Layer:**
- `tests/unit/infrastructure/git/test_operations.py` - Git command wrappers (13 tests)
- `tests/unit/infrastructure/github/test_operations.py` - GitHub CLI and API operations (27 tests)
- `tests/unit/infrastructure/github/test_actions.py` - GitHub Actions helpers (17 tests)
- `tests/unit/infrastructure/filesystem/test_operations.py` - Filesystem utilities (27 tests)

**Application Layer:**
- `tests/unit/application/collectors/test_statistics.py` - Statistics models and collectors (44 tests)
- `tests/unit/application/formatters/test_table_formatter.py` - Table formatting utilities (19 tests)
- `tests/unit/application/services/test_pr_operations.py` - PR operations (21 tests)
- `tests/unit/application/services/test_task_management.py` - Task finding and marking (19 tests)
- `tests/unit/application/services/test_reviewer_management.py` - Reviewer capacity and assignment (16 tests)
- `tests/unit/application/services/test_project_detection.py` - Project detection from PRs and path resolution (17 tests)
- `tests/unit/application/services/test_artifact_operations.py` - Artifact operations API (31 tests)

**CLI Layer:**
- `tests/unit/cli/commands/test_prepare_summary.py` - PR summary command (9 tests)
- `tests/unit/cli/commands/test_prepare.py` - Preparation workflow command (19 tests)
- `tests/unit/cli/commands/test_finalize.py` - Finalization workflow command (27 tests)
- `tests/unit/cli/commands/test_discover.py` - Project discovery command (16 tests)
- `tests/unit/cli/commands/test_discover_ready.py` - Ready project discovery command (18 tests)
- `tests/unit/cli/commands/test_statistics.py` - Statistics reporting command (15 tests)
- `tests/unit/cli/commands/test_add_cost_comment.py` - Cost comment posting command (18 tests)
- `tests/unit/cli/commands/test_extract_cost.py` - Cost extraction command (24 tests)
- `tests/unit/cli/commands/test_notify_pr.py` - PR notification command (23 tests)

**Integration:**
- Demo repository: `claude-step-demo/tests/integration/test_workflow_e2e.py` - End-to-end workflow

### Coverage Gaps
The following modules lack unit tests:

**CLI Layer:**
- ‚úÖ All CLI commands have comprehensive tests

## Testing Principles to Follow

Based on Python testing best practices:

1. **Test Isolation** - Each test is independent with clean state
2. **Fast Execution** - Mock external dependencies (GitHub API, git commands, file system where appropriate)
3. **One Concept Per Test** - Each test validates one specific behavior
4. **Descriptive Names** - Test names explain the scenario being tested
5. **Arrange-Act-Assert** - Clear test structure
6. **Parametrization** - Use pytest.mark.parametrize for similar test cases
7. **Proper Mocking** - Mock external services, not internal logic
8. **Edge Case Coverage** - Test boundary conditions, empty inputs, errors

**IMPORTANT**: All tests in this project MUST follow the style guide and conventions documented below. Consistency across the test suite is critical for maintainability.

## Test Style Guide and Conventions

### Required Test Structure

All tests MUST follow this structure:

```python
class TestFeatureName:
    """Test suite for FeatureName functionality"""

    @pytest.fixture
    def fixture_name(self):
        """Fixture providing test data - clear docstring explaining purpose"""
        return setup_test_data()

    def test_feature_does_something_when_condition(self, fixture_name):
        """Test description explaining what this verifies

        Should read like: "Test that feature does X when Y condition exists"
        """
        # Arrange - Set up test data and mocks
        expected_value = "expected"
        mock_dependency = Mock()

        # Act - Execute the code under test
        result = function_under_test(fixture_name, mock_dependency)

        # Assert - Verify the outcome
        assert result == expected_value
        mock_dependency.method.assert_called_once()
```

### Naming Conventions

**Class Names**: `TestModuleName` or `TestFunctionName`
- ‚úÖ `TestReviewerManagement`
- ‚úÖ `TestCheckReviewerCapacity`
- ‚ùå `ReviewerTests` (wrong suffix)
- ‚ùå `TestReviewers` (too vague)

**Test Method Names**: `test_<what>_<when>_<condition>`
- ‚úÖ `test_find_reviewer_returns_none_when_all_at_capacity`
- ‚úÖ `test_create_branch_raises_error_when_branch_exists`
- ‚úÖ `test_parse_branch_extracts_project_and_index`
- ‚ùå `test_reviewer` (not descriptive)
- ‚ùå `test_find_reviewer_works` (vague)
- ‚ùå `test_1` or `test_case_a` (meaningless)

**Fixture Names**: `<resource>_<state>` or `mock_<service>`
- ‚úÖ `reviewer_config`, `sample_spec_file`, `mock_github_api`
- ‚ùå `data`, `setup`, `fixture1`

### Good Test Patterns (‚úÖ WRITE THESE)

#### 1. Test Behavior, Not Implementation

```python
# ‚úÖ GOOD - Tests the behavior/outcome
def test_find_available_reviewer_returns_reviewer_under_capacity(mock_github_api):
    """Should return first reviewer with available capacity"""
    # Arrange
    reviewers = [
        ReviewerConfig(username="alice", maxOpenPRs=2),
        ReviewerConfig(username="bob", maxOpenPRs=2)
    ]
    mock_github_api.get_open_prs.side_effect = [
        [{"number": 1}, {"number": 2}],  # alice at capacity
        [{"number": 3}]  # bob under capacity
    ]

    # Act
    result = find_available_reviewer(reviewers, mock_github_api)

    # Assert
    assert result.username == "bob"

# ‚ùå BAD - Tests implementation details
def test_find_available_reviewer_calls_check_capacity_for_each(mock_check):
    """Don't test that internal functions are called - test outcomes"""
    reviewers = [ReviewerConfig(username="alice", maxOpenPRs=2)]

    with patch('module.check_reviewer_capacity') as mock_check:
        find_available_reviewer(reviewers)
        assert mock_check.call_count == 1  # Brittle - breaks if refactored
```

#### 2. Use Descriptive Assertions

```python
# ‚úÖ GOOD - Clear what's being tested and why
def test_format_branch_name_creates_correct_format():
    """Should create branch name in format: claude-step-{project}-{index}"""
    result = format_branch_name("my-project", 5)

    assert result == "claude-step-my-project-5"
    assert result.startswith("claude-step-")
    assert result.endswith("-5")

# ‚ùå BAD - Unclear what's being verified
def test_format_branch_name():
    result = format_branch_name("my-project", 5)
    assert result  # What does this even verify?
    assert len(result) > 0  # Too vague
```

#### 3. Test Edge Cases and Boundaries

```python
# ‚úÖ GOOD - Tests multiple boundary conditions
@pytest.mark.parametrize("open_pr_count,max_prs,expected", [
    (0, 2, True),   # Well under capacity
    (1, 2, True),   # Just under capacity
    (2, 2, False),  # Exactly at capacity
    (3, 2, False),  # Over capacity
    (0, 0, False),  # Edge case: zero max
])
def test_check_reviewer_capacity_boundary_conditions(
    open_pr_count, max_prs, expected, mock_github_api
):
    """Should correctly handle all capacity boundary conditions"""
    reviewer = ReviewerConfig(username="alice", maxOpenPRs=max_prs)
    mock_github_api.get_open_prs.return_value = [
        {"number": i} for i in range(open_pr_count)
    ]

    result = check_reviewer_capacity(reviewer, mock_github_api)

    assert result == expected
```

#### 4. Mock at System Boundaries

```python
# ‚úÖ GOOD - Mock external services (GitHub, git, filesystem)
def test_create_pull_request_success(mock_subprocess):
    """Should execute gh CLI command with correct arguments"""
    mock_subprocess.run.return_value = Mock(returncode=0, stdout="PR #123")

    result = create_pull_request(
        title="Test PR",
        body="Description",
        base="main"
    )

    assert result == "123"
    mock_subprocess.run.assert_called_once()
    args = mock_subprocess.run.call_args[0][0]
    assert "gh pr create" in " ".join(args)

# ‚ùå BAD - Mocking internal business logic
def test_prepare_command():
    with patch('module.find_next_task') as mock_find:  # Internal function
        with patch('module.check_capacity') as mock_check:  # Internal function
            # Too many mocks of internal logic = testing implementation
            cmd_prepare(args)
```

#### 5. Clear Test Data Setup

```python
# ‚úÖ GOOD - Clear, minimal test data
def test_parse_branch_name_extracts_project_and_index():
    """Should parse standard branch format correctly"""
    # Arrange
    branch_name = "claude-step-my-project-42"

    # Act
    project, index = parse_branch_name(branch_name)

    # Assert
    assert project == "my-project"
    assert index == 42

# ‚ùå BAD - Overly complex or irrelevant test data
def test_parse_branch_name():
    branch = "claude-step-" + "-".join([
        "my", "project", "with", "lots", "of", "parts"
    ]) + "-" + str(int("42"))  # Why so complex?
    project, index = parse_branch_name(branch)
    assert project  # Assertion doesn't even verify the parsing!
```

### Bad Test Patterns (‚ùå AVOID THESE)

#### 1. Tests Without Assertions (Test Smoke)

```python
# ‚ùå BAD - No assertions, just checking it doesn't crash
def test_load_config():
    """This doesn't test anything meaningful"""
    config = load_config("config.yml")
    # No assertions - what are we verifying?

# ‚úÖ GOOD - Actually verify the behavior
def test_load_config_parses_reviewers_correctly():
    """Should parse reviewer configuration from YAML"""
    config = load_config("test_config.yml")

    assert len(config.reviewers) == 2
    assert config.reviewers[0].username == "alice"
    assert config.reviewers[0].maxOpenPRs == 2
```

#### 2. Testing Language/Framework Features

```python
# ‚ùå BAD - Testing Python itself
def test_list_append():
    """Don't test that Python lists work"""
    my_list = []
    my_list.append(1)
    assert len(my_list) == 1

# ‚ùå BAD - Testing library behavior
def test_yaml_loads():
    """Don't test that PyYAML works"""
    result = yaml.safe_load("key: value")
    assert result == {"key": "value"}

# ‚úÖ GOOD - Test YOUR code
def test_config_validates_required_fields():
    """Should raise error when required fields are missing"""
    with pytest.raises(ConfigValidationError, match="reviewers"):
        load_config("config_without_reviewers.yml")
```

#### 3. Over-Mocking (Too Many Mocks)

```python
# ‚ùå BAD - Mocking everything = testing nothing
def test_prepare_command():
    with patch('module.os') as mock_os:
        with patch('module.Path') as mock_path:
            with patch('module.yaml') as mock_yaml:
                with patch('module.find_task') as mock_find:
                    with patch('module.create_branch') as mock_branch:
                        # 5+ mocks = you're testing mocks, not code
                        cmd_prepare(args)
                        mock_branch.assert_called_once()

# ‚úÖ GOOD - Mock boundaries, test logic
def test_prepare_command_creates_branch_with_correct_name(
    mock_git_client,
    mock_task_service,
    sample_config
):
    """Should create branch using format: claude-step-{project}-{index}"""
    # Arrange
    mock_task_service.find_next_task.return_value = Task(index=5, description="...")

    # Act
    cmd_prepare(args, mock_git_client, mock_task_service, sample_config)

    # Assert
    mock_git_client.create_branch.assert_called_once_with(
        "claude-step-my-project-5"
    )
```

#### 4. Testing Multiple Things (No Focus)

```python
# ‚ùå BAD - Testing too many things at once
def test_prepare_workflow():
    """Too broad - should be split into multiple tests"""
    result = cmd_prepare(args)
    assert result.exit_code == 0
    assert result.branch_created
    assert result.task_found
    assert result.reviewer_assigned
    assert result.prompt_generated
    # If any assertion fails, you don't know which part broke

# ‚úÖ GOOD - One concept per test
def test_prepare_creates_branch():
    """Should create a new branch for the task"""
    # Test only branch creation

def test_prepare_finds_next_task():
    """Should find the next uncompleted task from spec.md"""
    # Test only task finding

def test_prepare_assigns_reviewer():
    """Should assign a reviewer with available capacity"""
    # Test only reviewer assignment
```

#### 5. Brittle Tests (Coupled to Implementation)

```python
# ‚ùå BAD - Breaks when internal implementation changes
def test_find_task():
    with patch('module.open', mock_open(read_data="data")) as mock_file:
        find_next_task("spec.md")
        mock_file.assert_called_once_with("spec.md", "r")  # Brittle
        # Breaks if we change from 'r' to 'rt' or use Path.read_text()

# ‚úÖ GOOD - Tests behavior, not how it's implemented
def test_find_next_task_returns_first_uncompleted(tmp_path):
    """Should return the first unchecked task from spec file"""
    # Arrange
    spec_file = tmp_path / "spec.md"
    spec_file.write_text("""
    - [x] Completed task
    - [ ] Next task to do
    - [ ] Future task
    """)

    # Act
    result = find_next_task(str(spec_file))

    # Assert
    assert result.description == "Next task to do"
    assert result.index == 2
```

### Low-Value Tests (‚ùå DON'T WRITE)

#### 1. Getter/Setter Tests

```python
# ‚ùå DON'T WRITE - Pointless, no business logic
def test_reviewer_config_sets_username():
    """Don't test trivial assignments"""
    config = ReviewerConfig(username="alice", maxOpenPRs=2)
    assert config.username == "alice"
    assert config.maxOpenPRs == 2
```

#### 2. Tests That Duplicate the Code

```python
# ‚ùå DON'T WRITE - Just reimplements the function
def test_calculate_completion():
    """This is just duplicating the implementation"""
    completed = 3
    total = 10
    result = calculate_completion_percentage(completed, total)
    assert result == (completed / total) * 100  # Same logic as function!

# ‚úÖ WRITE THIS INSTEAD - Test edge cases and behavior
def test_calculate_completion_handles_zero_total():
    """Should return 0% when there are no tasks"""
    result = calculate_completion_percentage(0, 0)
    assert result == 0.0  # Not undefined or NaN
```

#### 3. Tests for Third-Party Code

```python
# ‚ùå DON'T WRITE - Testing PyYAML
def test_yaml_parsing():
    """Don't test libraries - they have their own tests"""
    data = yaml.safe_load("key: value")
    assert isinstance(data, dict)
```

### Test Code Smells (‚ö†Ô∏è WARNING SIGNS)

1. **No Assertions** - Test executes code but doesn't verify anything
2. **Magic Numbers** - Unexplained values (use constants with names)
3. **Sleep/Wait** - Tests shouldn't depend on timing (use mocks)
4. **Shared State** - Tests modify global state or depend on test order
5. **Too Many Mocks** - More than 3-4 mocks suggests poor boundaries
6. **Long Tests** - Over 20 lines suggests testing too much at once
7. **Duplicate Logic** - Test reimplements the code it's testing
8. **Vague Names** - Can't tell what's being tested from the name
9. **No Docstrings** - Purpose of test isn't clear
10. **Fixture Bloat** - Fixtures setting up data not used by the test

### Summary: Quick Checklist

Before committing a test, verify:

- [ ] Test name clearly describes what's being tested
- [ ] Test has a docstring explaining its purpose
- [ ] Test follows Arrange-Act-Assert structure
- [ ] Test has meaningful assertions (not just "doesn't crash")
- [ ] Test mocks external dependencies, not internal logic
- [ ] Test is focused (one concept/behavior)
- [ ] Test will fail if the code is broken
- [ ] Test won't break if internal implementation changes
- [ ] Test doesn't duplicate existing test coverage
- [ ] Test adds value (not testing language/framework/library)

## Implementation Plan

### Completed Phases

All core test implementation work has been completed (December 27, 2025). The project now has 493 passing tests with 85.03% coverage.

#### Phase 1: Infrastructure & Core Utilities ‚úÖ

- [x] **Set up test infrastructure**
  - [x] Add `pytest.ini` configuration file
  - [x] Set up code coverage reporting with `pytest-cov` in CI
  - [x] Tests run automatically in CI/CD (`.github/workflows/test.yml`)
  - [x] Package structure supports testing (`pyproject.toml` configured)

- [x] **Create common test fixtures** (`tests/conftest.py`)
  - [x] Fixture for temporary git repository (`mock_git_repo`)
  - [x] Fixture for mock GitHub API responses (`mock_github_api`, `sample_pr_data`, `sample_pr_list`)
  - [x] Fixture for sample project configurations (`sample_config_file`, `sample_config_dict`, `single_reviewer_config`, `no_reviewers_config`)
  - [x] Fixture for spec.md files with various states (`sample_spec_file`, `empty_spec_file`, `all_completed_spec_file`)
  - [x] Fixture for mocked GitHubActionsHelper (`mock_github_actions_helper`, `github_env_vars`)
  - [x] Additional fixtures: `tmp_project_dir`, `mock_subprocess`, `config_with_deprecated_field`, `sample_reviewer_config`, `sample_task_metadata`, `sample_prompt_template`

- [x] **Test domain layer** (`tests/unit/domain/`)
  - [x] Test `exceptions.py` - Custom exception classes and inheritance (17 tests)
  - [x] Test `models.py` - Domain model validation and serialization (37 tests)
  - [x] Test `config.py` - Configuration loading, validation, and branchPrefix rejection (26 tests)

#### Phase 2: Infrastructure Layer ‚úÖ

- [x] **Test pr_operations.py** (`tests/unit/application/services/test_pr_operations.py`)
  - [x] 21 comprehensive tests covering branch name generation, parsing, and PR fetching

- [x] **Test git operations** (`tests/unit/infrastructure/git/test_operations.py`)
  - [x] 13 tests covering git command execution and error handling

- [x] **Test GitHub operations** (`tests/unit/infrastructure/github/test_operations.py`)
  - [x] 27 tests covering GitHub CLI and API operations

- [x] **Test GitHub Actions helpers** (`tests/unit/infrastructure/github/test_actions.py`)
  - [x] 17 tests covering GitHub Actions environment integration

- [x] **Test filesystem operations** (`tests/unit/infrastructure/filesystem/test_operations.py`)
  - [x] 27 tests covering filesystem utilities

#### Phase 3: Application Services Layer ‚úÖ

- [x] **Test task_management.py** (`tests/unit/application/services/test_task_management.py`)
  - [x] 19 tests covering task finding, marking, ID generation

- [x] **Test statistics_collector.py** (`tests/unit/application/collectors/test_statistics.py`)
  - [x] 44 tests covering progress bars, task counting, team member stats, project stats

- [x] **Test table_formatter.py** (`tests/unit/application/formatters/test_table_formatter.py`)
  - [x] 19 tests covering visual width calculation, padding, emoji support, table formatting

- [x] **Test reviewer_management.py** (`tests/unit/application/services/test_reviewer_management.py`)
  - [x] 16 comprehensive tests covering reviewer capacity and assignment

- [x] **Test project_detection.py** (`tests/unit/application/services/test_project_detection.py`)
  - [x] 17 comprehensive tests covering project detection and path resolution

- [x] **Test artifact_operations.py** (`tests/unit/application/services/test_artifact_operations.py`)
  - [x] 31 comprehensive tests covering artifact operations API

#### Phase 4: CLI Commands Layer ‚úÖ

- [x] **Test prepare_summary.py** (`tests/unit/cli/commands/test_prepare_summary.py`)
  - [x] 9 tests covering prompt template loading and variable substitution

- [x] **Test prepare.py** (`tests/unit/cli/commands/test_prepare.py`)
  - [x] 19 comprehensive tests covering the full preparation workflow

- [x] **Test finalize.py** (`tests/unit/cli/commands/test_finalize.py`)
  - [x] 27 comprehensive tests covering the full finalization workflow

- [x] **Test discover.py** (`tests/unit/cli/commands/test_discover.py`)
  - [x] 16 comprehensive tests covering project discovery functionality

- [x] **Test discover_ready.py** (`tests/unit/cli/commands/test_discover_ready.py`)
  - [x] 18 comprehensive tests covering ready project discovery functionality

- [x] **Test statistics.py** (`tests/unit/cli/commands/test_statistics.py`)
  - [x] 15 comprehensive tests covering statistics report generation

- [x] **Test add_cost_comment.py** (`tests/unit/cli/commands/test_add_cost_comment.py`)
  - [x] 18 comprehensive tests covering cost comment posting functionality

- [x] **Test extract_cost.py** (`tests/unit/cli/commands/test_extract_cost.py`)
  - [x] 24 comprehensive tests covering cost extraction functionality

- [x] **Test notify_pr.py** (`tests/unit/cli/commands/test_notify_pr.py`)
  - [x] 23 comprehensive tests covering PR notification functionality

#### Phase 5: Integration & Quality ‚úÖ

- [x] **Improve existing tests**
  - [x] Added 7 new tests to `test_finalize.py` for uncovered edge cases
  - [x] Coverage improved from 83.84% to **85.03%**
  - [x] finalize.py coverage improved from 86.36% to **98.70%**

- [x] **Add integration test coverage**
  - [x] Integration testing covered by E2E tests in demo repository
  - [x] Unit tests provide excellent cross-layer coverage (493 tests, 85% coverage)

- [x] **Set up coverage reporting**
  - [x] Configure pytest-cov to track coverage (added to pytest.ini)
  - [x] Add coverage report to CI/CD pipeline
  - [x] Set minimum coverage threshold (70% in pyproject.toml, currently achieving 85.03%)
  - [x] Generate HTML coverage reports for local development (htmlcov/ directory)

#### Phase 6: Documentation ‚úÖ

- [x] **Document testing practices**
  - [x] Created `docs/testing-guide.md` with comprehensive testing practices
  - [x] Documented how to run tests locally
  - [x] Documented common fixtures and their usage from conftest.py
  - [x] Included examples of good and bad test patterns

- [x] **Set up basic CI/CD testing**
  - [x] Add GitHub Actions workflow for running tests on PR (`.github/workflows/test.yml`)
  - [x] Tests run on every push and PR to main branch

### Completed Work ‚úÖ

All planned work from this document has been completed:

#### Core Testing Implementation (Phases 1-5) ‚úÖ

- [x] Infrastructure & Core Utilities (Phase 1)
- [x] Infrastructure Layer (Phase 2)
- [x] Application Services Layer (Phase 3)
- [x] CLI Commands Layer (Phase 4)
- [x] Integration & Quality (Phase 5)
- [x] Documentation (Phase 6)

#### CI/CD Enhancements ‚úÖ

- [x] Add test status badge to README.md
- [x] Add coverage badge to README.md (static 85% badge)
- [x] Add coverage reporting to PR comments (automated comment with full report)
- [x] Configure PR merge requirements documentation (added to README Development section)
- [x] Identify and document intentionally untested code (`docs/testing-coverage-notes.md`)

### Next Steps

**This plan is complete.** All remaining work has been moved to:

üìã **[Test Coverage Improvement Plan - Phase 2](./test-coverage-improvement-2.md)**

The Phase 2 plan contains 10 optional enhancements organized into clear phases with detailed instructions for each task.

## Success Criteria

**‚úÖ Achieved Goals:**

- [x] **Coverage Goals**
  - [x] Overall coverage > 80% (currently 85.03%)
  - [x] All command modules tested (9/9 commands have comprehensive tests)
  - [x] All operations modules tested (git, github, filesystem all covered)

- [x] **Quality Goals**
  - [x] All tests are isolated and independent
  - [x] Test suite runs in < 10 seconds (currently ~0.7s)
  - [x] No flaky tests (493/493 passing consistently)
  - [x] Clear test names and documentation (all follow style guide)
  - [x] Proper use of fixtures and parametrization

- [x] **Process Goals**
  - [x] Tests run automatically on every PR
  - [x] Testing guide documented and accessible (`docs/testing-guide.md`)

**üîÑ In Progress:**

- [ ] New code requires tests (enforced in PR reviews via process, not automated - requires team adoption)

**‚úÖ Recently Completed:**

- [x] Coverage reports visible in PR reviews (automated PR comments with full coverage report)
- [x] Intentionally untested code documented (`docs/testing-coverage-notes.md`)
- [x] PR merge requirements documented in README

**üìã Deferred (Low ROI):**

- [ ] All business logic modules > 90% coverage (statistics_collector.py at 15%, intentionally tested via integration - see `docs/testing-coverage-notes.md`)

## Testing Tools & Dependencies

```bash
# Core testing framework
pytest>=7.0.0

# Coverage reporting
pytest-cov>=4.0.0

# Mocking (built-in to Python 3.3+)
# unittest.mock

# Property-based testing (optional)
hypothesis>=6.0.0

# Performance benchmarking (optional)
pytest-benchmark>=4.0.0
```

## Example Test Structure

```python
# tests/test_reviewer_management.py
import pytest
from unittest.mock import Mock, patch

from claudestep.reviewer_management import check_reviewer_capacity
from claudestep.models import ReviewerConfig


class TestCheckReviewerCapacity:
    """Tests for reviewer capacity checking"""

    @pytest.fixture
    def reviewer_config(self):
        """Fixture providing a sample reviewer configuration"""
        return ReviewerConfig(username="alice", maxOpenPRs=2)

    @pytest.fixture
    def mock_github_api(self):
        """Fixture providing mocked GitHub API"""
        with patch('claudestep.github_operations.get_open_prs') as mock:
            yield mock

    def test_reviewer_under_capacity(self, reviewer_config, mock_github_api):
        """Should return True when reviewer has capacity"""
        # Arrange
        mock_github_api.return_value = [{"number": 1}]  # 1 open PR

        # Act
        result = check_reviewer_capacity(reviewer_config)

        # Assert
        assert result is True
        mock_github_api.assert_called_once_with("alice", label="claude-step")

    def test_reviewer_at_capacity(self, reviewer_config, mock_github_api):
        """Should return False when reviewer is at capacity"""
        # Arrange
        mock_github_api.return_value = [{"number": 1}, {"number": 2}]  # 2 open PRs

        # Act
        result = check_reviewer_capacity(reviewer_config)

        # Assert
        assert result is False

    @pytest.mark.parametrize("open_pr_count,expected", [
        (0, True),   # Under capacity
        (1, True),   # Under capacity
        (2, False),  # At capacity
        (3, False),  # Over capacity
    ])
    def test_capacity_boundaries(self, reviewer_config, mock_github_api, open_pr_count, expected):
        """Should correctly handle various capacity levels"""
        # Arrange
        mock_github_api.return_value = [{"number": i} for i in range(open_pr_count)]

        # Act
        result = check_reviewer_capacity(reviewer_config)

        # Assert
        assert result == expected
```

## Implementation Approach (Completed)

### Strategy: Incremental Implementation ‚úÖ

The implementation followed a bottom-up approach, successfully completed in December 2025:

1. **‚úÖ Infrastructure** (Phase 1) - Established testing patterns and fixtures
2. **‚úÖ Operations layer** (Phase 2) - Enabled mocking in higher layers
3. **‚úÖ Business logic** (Phase 3) - Tested with operations mocked
4. **‚úÖ Commands** (Phase 4) - Tested with full dependency mocking
5. **‚úÖ Quality improvements** (Phase 5) - Achieved 85% coverage
6. **‚úÖ Documentation** (Phase 6) - Complete testing guide and CI/CD

All priorities have been completed - all test modules identified as high, medium, and low priority have comprehensive test coverage.

## Risks & Mitigation

| Risk | Mitigation |
|------|------------|
| Tests become slow due to excessive mocking | Use fixtures efficiently, minimize I/O operations |
| Mocking makes tests brittle | Mock at system boundaries only, not internal functions |
| Coverage metrics without quality | Require code review of tests, enforce best practices |
| Tests don't catch real bugs | Combine unit tests with integration tests, use E2E tests for critical paths |
| Tests become outdated | Run tests in CI/CD, require tests for new features |

## Timeline Summary

**‚úÖ Completed (December 2025)**:
- **Phase 1**: Infrastructure & Core Utilities - 100% complete
- **Phase 2**: Infrastructure Layer - 100% complete
- **Phase 3**: Application Services Layer - 100% complete
- **Phase 4**: CLI Commands Layer - 100% complete
- **Phase 5**: Integration & Quality - 100% complete
- **Phase 6**: Documentation - 100% complete

**Current Status**: 493 passing tests, 85.03% coverage, comprehensive test infrastructure in place.

**üìã Remaining Work**: CI/CD enhancements and optional improvements (see "Remaining Work" section above)

## Resources

- [pytest documentation](https://docs.pytest.org/)
- [pytest-cov documentation](https://pytest-cov.readthedocs.io/)
- [Python unittest.mock guide](https://docs.python.org/3/library/unittest.mock.html)
- [Hypothesis documentation](https://hypothesis.readthedocs.io/)
- [Testing Best Practices (Python Guide)](https://docs.python-guide.org/writing/tests/)

## Next Steps

**‚úÖ Completed Core Implementation:**
1. [x] Review and approve this proposal
2. [x] Set up initial testing infrastructure (Phase 1)
3. [x] Implement all unit tests (Phases 1-5)
4. [x] Document testing practices (Phase 6)
5. [x] Set up basic CI/CD workflow

**üìã Recommended Next Actions** (optional enhancements):
1. Set up CI workflow to run e2e integration tests from demo repository
2. Add test coverage reporting to PR comments
3. Add test status and coverage badges to README.md
4. Configure Python multi-version testing (3.11, 3.12, 3.13)
5. Consider property-based testing for critical paths (hypothesis library)

## Progress Summary

**‚úÖ All Core Work Complete (December 2025)**

The test coverage improvement plan has been successfully completed with all 6 phases done:

**Key Achievements:**
- **493 tests passing** (0 failures, up from 112 initially)
- **85.03% code coverage** (exceeding 70% minimum threshold)
- **Comprehensive test infrastructure** with 20+ reusable fixtures
- **All layers tested**: Domain (80 tests), Infrastructure (84 tests), Application (164 tests), CLI (165 tests)
- **Fast test execution**: ~0.7s for full suite
- **CI/CD integration**: Tests run automatically on every PR
- **Complete documentation**: `docs/testing-guide.md` (400+ lines)

**Test Breakdown by Layer:**
- Domain Layer: 80 tests (exceptions, models, config)
- Infrastructure Layer: 84 tests (git, github operations, filesystem)
- Application Layer: 164 tests (task management, statistics, reviewers, artifacts, projects)
- CLI Commands: 165 tests (all 9 commands fully tested)

**Coverage Highlights:**
- finalize.py: 98.70% coverage
- All 9 CLI commands have comprehensive test coverage
- All infrastructure operations tested with proper mocking
- Edge cases and error paths thoroughly tested

For detailed implementation notes and technical details, see the "Completed Phases" section above.
